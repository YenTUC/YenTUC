{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Stock Market Data\n",
    "\n",
    "The stockmarket data shows the daily percentage returns for the S&P 500 stock index between 2001 and 2005. \n",
    "It contains 1250 observations on the following 9 variables. Given we have so many variables we are operating in a high dimensional space, so don't be disappointed that there are no nice 2D plots with dicision boundaries. This time we have to trust the metrics which we learned in class to find out what is best.\n",
    "\n",
    "Features:\n",
    "\n",
    "- Year: The year that the observation was recorded\n",
    "\n",
    "- Lag1: Percentage return for previous day\n",
    "\n",
    "- Lag2: Percentage return for 2 days previous\n",
    "\n",
    "- Lag3: Percentage return for 3 days previous\n",
    "\n",
    "- Lag4: Percentage return for 4 days previous\n",
    "\n",
    "- Lag5: Percentage return for 5 days previous\n",
    "\n",
    "- Volume: Volume of shares traded (number of daily shares traded in billions)\n",
    "\n",
    "- Today: Percentage return for today\n",
    "\n",
    "Response:\n",
    "\n",
    "- Direction: A factor with levels Down and Up indicating whether the market had a positive or negative return on a given day.\n",
    "\n",
    "Given we only want to predict the ups and downs of the market value, it is a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import math\n",
    "from patsy import dmatrices\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sma\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket = pd.read_csv('data/Smarket.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Smarket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for panda data frame, there is a method corr to compute pairwise correlation between numerical variables\n",
    "Smarket.corr()\n",
    "# as one would expect, the correlations between the lag variables and today’s returns are close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at volume column\n",
    "plt.plot(Smarket.iloc[:, 6])\n",
    "# or plt.plot(Smarket[['Volume']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket, return_type = 'dataframe')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are more interested in stock marketing up, we take the second column of y as our response variable \n",
    "# we build a model to predict whether the direction will be up. \n",
    "logit = sm.Logit(y.iloc[:,1], X)\n",
    "logit.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract the parameters directly\n",
    "logit.fit().params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract the probability of the market going up for the first 10 instances\n",
    "logit.fit().predict()[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to make a prediction as to whether the market will go up or down on a particular day, \n",
    "# we must convert these predicted probabilities into class labels, Up (1) or Down (0).\n",
    "# we will do this by threshold the probability by a predefined threshold \n",
    "threshold = 0.5 \n",
    "predict_label = pd.DataFrame(np.zeros(shape=(1250,1)), columns = ['label'])\n",
    "predict_label.iloc[logit.fit().predict()>threshold] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can evalue the TRAINING result by constructing a confusion matrix \n",
    "confusion_matrix(y.iloc[:,1], predict_label.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. \n",
    "# in this case, logistic regression correctly predicted the movement of the market 52.2% of the time.\n",
    "print(np.mean(y.iloc[:,1] == predict_label.iloc[:,0]))\n",
    "# or use the confusion matrix to compute the accuracy \n",
    "print(confusion_matrix(y.iloc[:,1], predict_label.iloc[:,0]).diagonal().sum()* 1.0 /confusion_matrix(y.iloc[:,1], predict_label.iloc[:,0]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to better assess the accuracy of the logistic regression model in this setting, \n",
    "# we can fit the model using part of the data, and then examine how well it predicts the hold out data. \n",
    "# this will yield a more realistic error rate, in the sense that in practice we will be interested in our \n",
    "# model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown.\n",
    "Smarket_2005 = Smarket.query('Year >= 2005')\n",
    "Smarket_train = Smarket.query('Year < 2005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the training dataset to build the logistic regression model \n",
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_2005, return_type = 'dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = sm.Logit(y_train.iloc[:,1], X_train)\n",
    "print(logit.fit().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.5\n",
    "mark = (preds > threshold).reset_index(drop=True)\n",
    "predict_label.loc[mark] = 1\n",
    "confusion_matrix(y_test.iloc[:,1], predict_label.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get accuracy\n",
    "np.mean(y_test.iloc[:,1].reset_index(drop=True)==predict_label.iloc[:,0].reset_index(drop=True)) \n",
    "\n",
    "# note: we have trained and tested our model on two completely separate data sets: \n",
    "# training was performed using only the dates before 2005, and testing was performed \n",
    "# using only the dates in 2005. Finally, we compute the predictions for 2005 and compare \n",
    "# them to the actual movements of the market over that time period. The results are rather \n",
    "# disappointing: the test error rate is 1 - 48% = 52 %, which is worse than random guessing \n",
    "# for a balanced data. Of course this result is not all that surprising, given that one \n",
    "# would not generally expect to be able to use previous days’ returns to predict future market performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the retrain of the model with Lag1 and Lag2 will be similar to previous steps (I will be brief here). \n",
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2', Smarket_2005, return_type = 'dataframe')\n",
    "logit = sm.Logit(y_train.iloc[:,1], X_train)\n",
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.5\n",
    "confusion_matrix(y_test.iloc[:,1], predict_label.iloc[:,0])\n",
    "np.mean(y_test.iloc[:,1].reset_index(drop=True)==predict_label.iloc[:,0].reset_index(drop=True)) # to get accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to deal with logistics regression is to change the threshold value from 0.5 to others. \n",
    "# there is an example below with threshold 0.45. \n",
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.45\n",
    "predict_label.loc[(preds > threshold).reset_index(drop=True)] = 1\n",
    "confusion_matrix(y_test.iloc[:,1], predict_label.iloc[:,0])\n",
    "\n",
    "# to get accuracy on validation set, we did see an improvment of the accuracy from 0.48 to 0.56\n",
    "np.mean(y_test.iloc[:,1].reset_index(drop=True)==predict_label.iloc[:,0].reset_index(drop=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use sklearn's implementation of LDA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.iloc[:,1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training process \n",
    "sklearn_lda = LDA(n_components=1) #creating a LDA object\n",
    "lda = sklearn_lda.fit(X_train.iloc[:,1:3], y_train.iloc[:,1]) #learning the projection matrix\n",
    "X_lda = lda.transform(X_train.iloc[:,1:3]) #using the model to project X \n",
    "X_labels = lda.predict(X_train.iloc[:,1:3]) #gives you the predicted label for each sample\n",
    "X_prob = lda.predict_proba(X_train.iloc[:,1:3]) #the probability of each sample to belong to each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing step \n",
    "X_test_labels =lda.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = lda.predict_proba(X_test.iloc[:,1:3]) \n",
    "print(X_test_prob[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy of the test set using default threshold\n",
    "np.mean(y_test.iloc[:,1]==X_test_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's change the threshod a bit to see whether we can improve the accuracy. \n",
    "# the 2nd column of X_test_prob is the probability belongs to UP group. \n",
    "# the default value is 0.5, let us first check that. \n",
    "threshold = 0.5 \n",
    "np.mean(y_test.iloc[:,1]==(X_test_prob[:,1]>=threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.48\n",
    "np.mean(y_test.iloc[:,1]==(X_test_prob[:,1]>=threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is a little bit of annoying that QDA and LDA have minor difference in their parameter \n",
    "# set-up and function names. \n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sklearn_qda = QDA(priors=None,store_covariance=True) #creating a QDA object\n",
    "qda = sklearn_qda.fit(X_train.iloc[:,1:3], y_train.iloc[:,1]) #learning the projection matrix\n",
    "X_labels = qda.predict(X_train.iloc[:,1:3]) #gives you the predicted label for each sample\n",
    "X_prob = qda.predict_proba(X_train.iloc[:,1:3]) #the probability of each sample to belong to each class\n",
    "\n",
    "X_test_labels=qda.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = qda.predict_proba(X_test.iloc[:,1:3]) \n",
    "\n",
    "print(np.mean(y_test.iloc[:,1]==X_test_labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, use dir() to explore all the information stored in lda and qda.\n",
    "#dir(qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qda.means_)\n",
    "print(qda.covariance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB as NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_class = NB()\n",
    "NB_class.fit(X_train.iloc[:,1:3], y_train.iloc[:,1])\n",
    "X_test_labels=NB_class.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = NB_class.predict_proba(X_test.iloc[:,1:3]) \n",
    "print(np.mean(y_test.iloc[:,1]==X_test_labels))\n",
    "\n",
    "#dir(NB_class) # use dir command to check what Naive Bayes classifier has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNN(n_neighbors= 4) # use n_neighbors to change the # of tune the performance of KNN\n",
    "KNN_fit = neigh.fit(X_train.iloc[:,1:3], y_train.iloc[:,1]) #learning the projection matrix\n",
    "X_test_labels=KNN_fit.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = KNN_fit.predict_proba(X_test.iloc[:,1:3]) \n",
    "print(np.mean(y_test.iloc[:,1]==X_test_labels))\n",
    "\n",
    "#dir(neigh) # use dir command to check what KNN offers"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "4548a0e672c5b3a287feee7b2962606840aa548749d1830ef724408652b0c250"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
